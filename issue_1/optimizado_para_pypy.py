import os, mmap
from collections import defaultdict
from concurrent.futures import ProcessPoolExecutor

FILE = "./measurements.txt"
NP = os.cpu_count() # N칰mero de cores (incluye smt)

def consumer(start_pos, end_pos):
    # Vamos a leer el archivo en modo binario y vamos a tratar todo como bytes y
    # no como texto / unicode (utf-8) porque decodificar bytes a utf-8 es
    # carisimo y no est치 el paiton para menearlo.
    f = open(FILE, "rb")

    # mmapear un archivo con offset distinto a 0 requiere alinearlo al tama침o de
    # la pagina del kernel... Explicado para frontenders:
    # Supongamos que start_pos era 68. No podemos situarnos directamente ah칤
    # porque el offset de mmap necesita un multiplo del tama침o de p치gina del
    # kernel. En mi caso (en mi maquina), el tama침o de p치gina es de 16, as칤 que
    # me muevo a 64 (la direcci칩n mas cercana a 68, multiplo de 16).
    mem_align = (start_pos // mmap.PAGESIZE) * mmap.PAGESIZE

    offset = start_pos - mem_align
    length = end_pos - start_pos + offset
    fm = mmap.mmap(f.fileno(), length=length, access=mmap.ACCESS_READ, offset=mem_align)

    # Ya que estamos mmapeando archivos, hay un truco para optimizar la lectura.
    # Podemos advertirle al kernel que vamos a leer el archivo de manera
    # secuencial y, por lo tanto, puede asumir que puede ir leyendo cosas a las
    # que todav칤a no hemos llegado para ahorrar tiempo.
    # ... todo esto si MacOS no fuese una gran putisima 游눨, claro. *sigh*
    # Tim Apple dimisi칩n!!
    fm.madvise(mmap.MADV_SEQUENTIAL, 0, length)
    fm.madvise(mmap.MADV_WILLNEED, 0, length)

    # ... y como hemos retrocedido hac칤a atras, ahora tenemos que compensar con
    # la diferencia entre lo que hemos retrocedido y la posici칩n donde realmente
    # deber칤amos estar
    fm.seek(offset)

    # Vamos a guardar los resultados en un diccionario donde el key va a ser el
    # churro binario y el valor va a ser un array de 4 n칰meros, representando el
    # m칤nimo, el m치ximo, la media (agregado) y el n칰mero de veces que aparece,
    # (para poder sacar la media).
    # 쯏 por que no un diccionario para el valor? Porque la complejidad de
    # acceso a un diccionario es avg O(1), mientras que el de un array es
    # siempre O(1). Y porque ocupa menos memoria.
    result = defaultdict(lambda: [0, 0, 0, 0])

    # Podr칤a parecer que la manera mas 칩ptima de iterar sobre las l칤neas del
    # archivo es con un iterador, pero un bucle infinito es mas 칩ptimo porque
    # va a generar una estructura de instrucciones con 0 comprobaciones
    # adicionales.
    #for line in iter(fm.readline, b""):

    # Micro-optimizaci칩n: cada vez que accedemos a atributo o metodo de un
    # objeto usando ".", Paiton tiene que acceder a una tabla de hashmap para
    # encontrarlo. Si nos guardamos una referencia a la funci칩n "readline()",
    # podemos ahorrarnos unos cuantos ciclos de CPU por cada iteraci칩n.
    _r = fm.readline
    while True:
        #chunk = fm.readline()
        chunk = _r()

        # Micro-optimizaci칩n: dado que estoy optimizando este c칩digo para PyPy y
        # no para CPython, puedo aprovecharme del hecho de que aqu칤 acceder a
        # posiciones aleatorios de bytes es m치s r치pido que generar 2 churros de
        # bytes distintos...
        sep = chunk.find(b";")

        try:
            # ... y tambi칠n me puedo aprovechar del hecho de que parsear un int
            # es m치s r치pido que parsear un float (al rev칠s que en CPython).
            # Y para parsear el int lo que voy a hacer es leer todo desde la
            # posici칩n del separador (";") hasta el final - 3 caracteres, y lo
            # voy a concatenar a los pen칰ltimos 2 caracteres.
            # B치sicamente, en el string "LaCiudad;-46.8\n" (aqu칤 tengo "\n" al
            # final del churro de bytes) leo "-46" por un lado y "8" por otro.
            # Voy a parsearlo como int y al final del todo, cuando est칠
            # imprimiendo los resultados, dividir칠 entre 10.
            v = int(chunk[sep+1:-3] + chunk[-2:-1], 10)
        except Exception:
            break

        # En cualquier caso, NO vamos a convertir el churro de bytes en string.
        # Eso es carisimo, no nos lo podemos permitir. Vamos a trabajar con
        # bytes en vez de con strings. Igualmente, 쯤uien necesita strings? Los
        # strings estan sobrevalorados.

        # No nos hace falta comprobar si "station" est치 en el diccionario porque
        # estamos usando un defaultdict, eso nos asegura que va a estar.
        _s = result[chunk[0:sep]]


        # Micro-optimizaci칩n: en vez de hacer algo as칤:
        #_s[0] = min(v, _s[0]) # min
        #_s[1] = max(v, _s[1]) # max
        # podemos usar nuestro propio c칩digo. min() y max() van a hacer 3
        # comparaciones (cada uno), mientras que el nuestro va a hacer 3 en
        # total. 쯇odr칤amos mover esto a una nueva funci칩n? Pues claro. Pero al
        # dejarlo aqu칤 nos ahorramos el *tremendo* overhead que supone llamar
        # una nueva funci칩n.
        #if _s[0] > _s[1]:
        #    _max = _s[0]
        #    _min = _s[1]
        #else:
        #    _max = _s[1]
        #    _min = _s[0]
        #if v > _max:
        #    _max = v
        #if v < _min:
        #    _min = v

        # Micro-optimizaci칩n: 쯇odr칤amos mover cada assign a una nueva l칤nea?
        #_s[0] = _min # min
        #_s[1] = _max # max
        #_s[2] = _s[2] + v # avg
        #_s[3] = _s[3] + 1 #count
        # Pues claro. Y nos quedar칤a la hostia de cuki. Pero aqu칤 hemos venido
        # a raspar cada ciclo de CPU que podemos, as칤 que vamos a meter todo en
        # 1 l칤nea para que el interprete haga los 4 assign en menos operaciones.
        #_s[0], _s[1], _s[2], _s[3] = _min, _max, _s[2] + v, _s[3] + 1

        # Micro-optimizaci칩n: Ok... podemos hacer todo lo anterior, pero podemos
        # optimizarlo todav칤a mas: Podemos no asignar los nuevos valores de min
        # y max en cada iteraci칩n, sino asignar solo los que han cambiado.
        # "bah... eso ahorra 1 if/else y 2 if, eso no es na..."
        # Te recuerdo, querido lector, que estamos en medio de un bucle que va a
        # procesar ~(1 bill칩n / NP) l칤neas. Cada instrucci칩n que nos ahorremos
        # va a agregar decenas de miles de ciclos a la ejecuci칩n.
        if v < _s[0]:
            _s[0] = v
        elif v > _s[1]:
            _s[1] = v
        _s[2], _s[3] = _s[2] + v, _s[3] + 1

    # Paiton cerrar치 esto por nosotros
    #fm.close()
    #f.close()

    # Vamos a devolver un array de arrays. Y cada array va a tener el nombre de
    # la ciudad, el min, el max y el avg. Y vamos a ordenarlos porque as칤 cada
    # proceso se encargar치 de ordenar ~(1 bill칩n / NP) elementos, distribuyendo
    # as칤 el trabajo.
    return [[k, v[0], v[1], v[2] / v[3]] for k, v in sorted(result.items())]
    # station^ min^  max^  avg^

def main():
    # Lo que queremos hacer es distribuir el trabajo entre todos los cores de
    # tal manera para que ninguno se quede sin trabajo. Leemos el tama침o del
    # archivo, lo dividimos entre NP y eso nos da el tama침o (aproximado*) de
    # datos que le daremos a cada proceso.
    file_size = os.path.getsize(FILE)
    chunk_size_per_thread = file_size // NP

    f = open(FILE, "rb")
    fm = mmap.mmap(f.fileno(), length=0, access=mmap.ACCESS_READ)

    workers = []
    start_pos = 0
    ppe = ProcessPoolExecutor()
    for i in range(NP):
        # *aproximado - si le diesemos a todos los procesos el mismo bloque de
        # datos, muy probablemente a alguno de los procesos le dar칤amos un
        # bloque de datos que no termina en "\n", sino que termina en medio de
        # los datos. Y eso provocar칤a que perdamos datos...
        # Lo que queremos hacer es ajustar el tama침o con unos bytes m치s, para
        # llegar al "\n" mas cercano.
        if i == NP - 1:
            end_pos = file_size
        else:
            end_pos = (fm.find(b"\n", (i + 1) * chunk_size_per_thread) + 1)
        workers.append(ppe.submit(consumer, start_pos, end_pos))
        start_pos = end_pos

    # Paiton cerrar치 esto por nosotros
    #fm.close()
    #f.close()

    # Sabemos que results est치 ordenado, as칤 que podemos optimizar el acceso a
    # los datos...
    output = []
    for result in zip(*[w.result() for w in workers]):
        # Lleg칩 el momento... vamos a pagar el coste de convertir bytes a utf-8.
        # Pero lo pagamos solo 1 vez por cada ciudad, no 1 bill칩n de veces.
        station = result[0][0].decode("utf-8")

        # ... usando map para que el interprete optimice esta secci칩n.
        mins, maxs, avgs = zip(*map(lambda v: (v[1], v[2], v[3]), result))

        output.append(f"{station}={0.1*min(mins):.1f}/{0.1*(sum(avgs) / len(avgs)):.1f}/{0.1*max(maxs):.1f}")
    print(f"{{{', '.join(output)}}}")

if __name__ == "__main__":
    main()